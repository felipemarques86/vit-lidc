{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lung Cancer Detection using Visual Transformers with LIDC-IDRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 10:02:32.030015: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pylidc as pl\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOTAL_ANNOTATIONS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total annotations:  6859\n"
     ]
    }
   ],
   "source": [
    "total_images = 0\n",
    "annotation_list = pl.query(pl.Annotation)\n",
    "print(\"Total annotations: \", annotation_list.count())\n",
    "annotations_count = min(annotation_list.count() , MAX_TOTAL_ANNOTATIONS)\n",
    "#\n",
    "#int(annotation_list.count() * 0.001)\n",
    "images = []\n",
    "annotations = []\n",
    "for i in range(0, annotations_count):\n",
    "    annotation = annotation_list[i]\n",
    "    annotation_bbox = annotation.bbox()\n",
    "    \n",
    "    vol = annotation.scan.to_volume(verbose=False)\n",
    "\n",
    "    y0, y1 = annotation_bbox[0].start, annotation_bbox[0].stop\n",
    "    x0, x1 = annotation_bbox[1].start, annotation_bbox[1].stop\n",
    "\n",
    "    # Get the central slice of the computed bounding box.\n",
    "    i, j, k = annotation.centroid\n",
    "    z = max(int(annotation_bbox[2].stop - k) - 1, 0)\n",
    "    (w, h) = vol[:, :, int(k)].shape\n",
    "    \n",
    "    # bbox = (slice(y0, y1), slice(x0,x1), slice(annotation_bbox[2].start, annotation_bbox[2].stop))\n",
    "    \n",
    "    scaled_bbox = (float(x0) / w, float(y0) / h, float(x1) / w, float(y1) / h)\n",
    "\n",
    "    #\n",
    "\n",
    "    #print(scaled_bbox)\n",
    "    \n",
    "    #plt.imshow(vol[:, :, int(z)], cmap=plt.cm.gray)\n",
    "    #plt.show()\n",
    "    \n",
    "    for j in range(annotation_bbox[2].start, annotation_bbox[2].stop):\n",
    "        annotations.append(scaled_bbox)\n",
    "        images.append(vol[:, :, j])\n",
    "        total_images = total_images + 1\n",
    "\n",
    "# Convert the list to numpy array, split to train and test dataset\n",
    "(xtrain), (ytrain) = (\n",
    "    np.asarray(images[: int(len(images) * 0.8)]),\n",
    "    np.asarray(annotations[: int(len(annotations) * 0.8)]),\n",
    ")\n",
    "(xtest), (ytest) = (\n",
    "    np.asarray(images[int(len(images) * 0.8):]),\n",
    "    np.asarray(annotations[int(len(annotations) * 0.8):]),\n",
    ")\n",
    "\n",
    "print(\"Total images used:\", total_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "for i in range(0, min(total_images, 10)):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 15))\n",
    "\n",
    "    (h, w) = (images[i]).shape[0:2]\n",
    "\n",
    "    y0, y1 = int(annotations[i][1]*h), int(annotations[i][3]*h)\n",
    "    x0, x1 = int(annotations[i][0]*w), int(annotations[i][2]*w)\n",
    "\n",
    "\n",
    "    top_left_x, top_left_y = x0, y0\n",
    "    bottom_right_x, bottom_right_y = x1, y1\n",
    "\n",
    "    box_truth = top_left_x, top_left_y, bottom_right_x, bottom_right_y\n",
    "    bbox = (slice(top_left_y, bottom_right_y), slice(top_left_x,bottom_right_x))\n",
    "    ax1.imshow(images[i], cmap=plt.cm.gray)\n",
    "\n",
    "    # Create the bounding box\n",
    "    rect = patches.Rectangle(\n",
    "        (top_left_x, top_left_y),\n",
    "        bottom_right_x - top_left_x,\n",
    "        bottom_right_y - top_left_y,\n",
    "        facecolor=\"none\",\n",
    "        edgecolor=\"red\",\n",
    "        linewidth=1,\n",
    "    )\n",
    "    # Add the bounding box to the image\n",
    "    ax1.add_patch(rect)\n",
    "    ax2.imshow(images[i][(slice(y0, y1), slice(x0, x1))], cmap=plt.cm.gray)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size, p_input_shape, p_num_patches, p_projection_dim, p_num_heads, p_transformer_units, p_transformer_layers, p_mlp_head_units):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.p_input_shape = p_input_shape\n",
    "        self.p_num_patches = p_num_patches\n",
    "        self.p_projection_dim = p_projection_dim\n",
    "        self.p_num_heads = p_num_heads\n",
    "        self.p_transformer_units = p_transformer_units\n",
    "        self.p_transformer_layers = p_transformer_layers\n",
    "        self.p_mlp_head_units = p_mlp_head_units\n",
    "\n",
    "    #     Override function to avoid error while saving model\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update(\n",
    "            {\n",
    "                \"input_shape\": self.p_input_shape,\n",
    "                \"patch_size\": self.patch_size,\n",
    "                \"num_patches\": self.p_num_patches,\n",
    "                \"projection_dim\": self.p_projection_dim,\n",
    "                \"num_heads\": self.p_num_heads,\n",
    "                \"transformer_units\": self.p_transformer_units,\n",
    "                \"transformer_layers\": self.p_transformer_layers,\n",
    "                \"mlp_head_units\": self.p_mlp_head_units,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        # return patches\n",
    "        return tf.reshape(patches, [batch_size, -1, patches.shape[-1]])\n",
    "\n",
    "\"\"\"## Implement the patch encoding layer\n",
    "\n",
    "The `PatchEncoder` layer linearly transforms a patch by projecting it into a\n",
    "vector of size `projection_dim`. It also adds a learnable position\n",
    "embedding to the projected vector.\n",
    "\"\"\"\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, p_patch_size, p_input_shape, p_projection_dim, p_num_heads, p_transformer_units, p_transformer_layers, p_mlp_head_units):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=p_projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=p_projection_dim\n",
    "        )\n",
    "        self.p_input_shape = p_input_shape\n",
    "        self.p_projection_dim = p_projection_dim\n",
    "        self.p_num_heads = p_num_heads\n",
    "        self.p_transformer_units = p_transformer_units\n",
    "        self.p_transformer_layers = p_transformer_layers\n",
    "        self.p_mlp_head_units = p_mlp_head_units\n",
    "        self.p_patch_size = p_patch_size\n",
    "\n",
    "    # Override function to avoid error while saving model\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update(\n",
    "            {\n",
    "                \"input_shape\": self.p_input_shape,\n",
    "                \"patch_size\": self.p_patch_size,\n",
    "                \"num_patches\": self.num_patches,\n",
    "                \"projection_dim\": self.p_projection_dim,\n",
    "                \"num_heads\": self.p_num_heads,\n",
    "                \"transformer_units\": self.p_transformer_units,\n",
    "                \"transformer_layers\": self.p_transformer_layers,\n",
    "                \"mlp_head_units\": self.p_mlp_head_units,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_vit_object_detector(\n",
    "    input_shape,\n",
    "    patch_size,\n",
    "    num_patches,\n",
    "    projection_dim,\n",
    "    num_heads,\n",
    "    transformer_units,\n",
    "    transformer_layers,\n",
    "    mlp_head_units,\n",
    "):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches\n",
    "    patches = Patches(patch_size, input_shape, num_patches, projection_dim, num_heads,\n",
    "                  transformer_units, transformer_layers, mlp_head_units)(inputs)\n",
    "    # Encode patches\n",
    "    encoded_patches = PatchEncoder(num_patches, patch_size, input_shape, projection_dim, num_heads,\n",
    "                  transformer_units, transformer_layers, mlp_head_units)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.3)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.3)\n",
    "\n",
    "    bounding_box = layers.Dense(4)(\n",
    "        features\n",
    "    )  # Final four neurons that output bounding box\n",
    "\n",
    "    # return Keras model.\n",
    "    return keras.Model(inputs=inputs, outputs=bounding_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(vit_object_detector, image_size, x_test, y_test):\n",
    "    import matplotlib.patches as patches\n",
    "\n",
    "    # Saves the model in current path\n",
    "    vit_object_detector.save(\"vit_object_detector.h5\", save_format=\"h5\")\n",
    "\n",
    "    i, mean_iou = 0, 0\n",
    "\n",
    "    # Compare results for 10 images in the test set\n",
    "    for input_image in x_test[:10]:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 15))\n",
    "        im = input_image\n",
    "\n",
    "        # Display the image\n",
    "        ax1.imshow(im, cmap=plt.cm.gray)\n",
    "        ax2.imshow(im, cmap=plt.cm.gray)\n",
    "\n",
    "        #input_image = cv2.resize(\n",
    "        #    input_image, (image_size, image_size), interpolation=cv2.INTER_AREA\n",
    "        #)\n",
    "        input_image = np.expand_dims(input_image, axis=0)\n",
    "        preds = vit_object_detector.predict(input_image)[0]\n",
    "\n",
    "        (h, w) = (im).shape[0:2]\n",
    "\n",
    "        top_left_x, top_left_y = int(preds[0] * w), int(preds[1] * h)\n",
    "\n",
    "        bottom_right_x, bottom_right_y = int(preds[2] * w), int(preds[3] * h)\n",
    "\n",
    "        box_predicted = [top_left_x, top_left_y, bottom_right_x, bottom_right_y]\n",
    "        # Create the bounding box\n",
    "        rect = patches.Rectangle(\n",
    "            (top_left_x, top_left_y),\n",
    "            bottom_right_x - top_left_x,\n",
    "            bottom_right_y - top_left_y,\n",
    "            facecolor=\"none\",\n",
    "            edgecolor=\"red\",\n",
    "            linewidth=1,\n",
    "        )\n",
    "        # Add the bounding box to the image\n",
    "        ax1.add_patch(rect)\n",
    "        ax1.set_xlabel(\n",
    "            \"Predicted: \"\n",
    "            + str(top_left_x)\n",
    "            + \", \"\n",
    "            + str(top_left_y)\n",
    "            + \", \"\n",
    "            + str(bottom_right_x)\n",
    "            + \", \"\n",
    "            + str(bottom_right_y)\n",
    "        )\n",
    "\n",
    "        top_left_x, top_left_y = int(y_test[i][0] * w), int(y_test[i][1] * h)\n",
    "\n",
    "        bottom_right_x, bottom_right_y = int(y_test[i][2] * w), int(y_test[i][3] * h)\n",
    "\n",
    "        box_truth = top_left_x, top_left_y, bottom_right_x, bottom_right_y\n",
    "\n",
    "        mean_iou += bounding_box_intersection_over_union(box_predicted, box_truth)\n",
    "        # Create the bounding box\n",
    "        rect = patches.Rectangle(\n",
    "            (top_left_x, top_left_y),\n",
    "            bottom_right_x - top_left_x,\n",
    "            bottom_right_y - top_left_y,\n",
    "            facecolor=\"none\",\n",
    "            edgecolor=\"red\",\n",
    "            linewidth=1,\n",
    "        )\n",
    "        # Add the bounding box to the image\n",
    "        ax2.add_patch(rect)\n",
    "        ax2.set_xlabel(\n",
    "            \"Target: \"\n",
    "            + str(top_left_x)\n",
    "            + \", \"\n",
    "            + str(top_left_y)\n",
    "            + \", \"\n",
    "            + str(bottom_right_x)\n",
    "            + \", \"\n",
    "            + str(bottom_right_y)\n",
    "            + \"\\n\"\n",
    "            + \"IoU\"\n",
    "            + str(bounding_box_intersection_over_union(box_predicted, box_truth))\n",
    "        )\n",
    "        i = i + 1\n",
    "\n",
    "    print(\"mean_iou: \" + str(mean_iou / len(x_test[:10])))\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "TEST_SIZE = 1 - TRAIN_SIZE\n",
    "IMAGE_SIZE = 512\n",
    "patch_size = 128  # Size of the patches to be extracted from the input images\n",
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, 1)  # input image shape\n",
    "learning_rate = 0.002\n",
    "weight_decay = 0.00001\n",
    "batch_size = 64\n",
    "num_epochs = 150\n",
    "num_patches = (IMAGE_SIZE // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 12\n",
    "# Size of the transformer layers\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]\n",
    "transformer_layers = 12\n",
    "mlp_head_units = [2048, 1024, 512, 64, 32]  # Size of the dense layers\n",
    "history = []\n",
    "num_patches = (IMAGE_SIZE // patch_size) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_object_detector = create_vit_object_detector(\n",
    "    input_shape,\n",
    "    patch_size,\n",
    "    num_patches,\n",
    "    projection_dim,\n",
    "    num_heads,\n",
    "    transformer_units,\n",
    "    transformer_layers,\n",
    "    mlp_head_units,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, learning_rate, weight_decay, batch_size, num_epochs, x_train, y_train):\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Compile model.\n",
    "    model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError())\n",
    "\n",
    "    checkpoint_filepath = \"../logs/\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[\n",
    "            checkpoint_callback\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = run_experiment(\n",
    "    vit_object_detector, learning_rate, weight_decay, batch_size, num_epochs, xtrain, ytrain\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate IoU (intersection over union, given two bounding boxes)\n",
    "def bounding_box_intersection_over_union(box_predicted, box_truth):\n",
    "    # get (x, y) coordinates of intersection of bounding boxes\n",
    "    top_x_intersect = max(box_predicted[0], box_truth[0])\n",
    "    top_y_intersect = max(box_predicted[1], box_truth[1])\n",
    "    bottom_x_intersect = min(box_predicted[2], box_truth[2])\n",
    "    bottom_y_intersect = min(box_predicted[3], box_truth[3])\n",
    "\n",
    "    # calculate area of the intersection bb (bounding box)\n",
    "    intersection_area = max(0, bottom_x_intersect - top_x_intersect + 1) * max(\n",
    "        0, bottom_y_intersect - top_y_intersect + 1\n",
    "    )\n",
    "\n",
    "    # calculate area of the prediction bb and ground-truth bb\n",
    "    box_predicted_area = (box_predicted[2] - box_predicted[0] + 1) * (\n",
    "        box_predicted[3] - box_predicted[1] + 1\n",
    "    )\n",
    "    box_truth_area = (box_truth[2] - box_truth[0] + 1) * (\n",
    "        box_truth[3] - box_truth[1] + 1\n",
    "    )\n",
    "\n",
    "    # calculate intersection over union by taking intersection\n",
    "    # area and dividing it by the sum of predicted bb and ground truth\n",
    "    # bb areas subtracted by  the interesection area\n",
    "\n",
    "    # return ioU\n",
    "    return intersection_area / float(\n",
    "        box_predicted_area + box_truth_area - intersection_area\n",
    "    )\n",
    "\n",
    "print_results(vit_object_detector, IMAGE_SIZE, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
